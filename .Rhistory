print(final_df %>% group_by(`question.id`) %>% summarise(response_count = n()))
print(final_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
print(analysis_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
print(n=34,final_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
print(n=34analysis_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
print(n=34, final_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
print(n=34, analysis_df %>% group_by(`base_condition`,`question_id`) %>% summarise(response_count = n()))
final_df_summary <- final_df %>%
group_by(base_condition, question_id) %>%
summarise(
response_count = n(),
simulation_ids = list(unique(Simulation.id))
)
View(final_df_summary)
list(unique(final_df$Simulation.id))
View(final_df_summary)
class(list(unique(final_df$Simulation.id)))
class((unique(final_df$Simulation.id)))
final_df_summary <- final_df %>%
group_by(base_condition, question_id) %>%
summarise(
response_count = n(),
simulation_ids = toString(unique(Simulation.id))
)
View(final_df_summary)
print(n=34, final_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id))))
print(n=34, analysis_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id))))
rm(list = ls())
#install.packages("httr")
library("httr")
library("data.table")
library("dplyr")
library("lubridate")
library("writexl")
library("jsonlite")
library("readxl")
library("stringr")
library("tidyr")
### -------------- STEP 1: SETUP AND DEFINE FUNCTIONS --------------
# ---- SETUP
# a) Set current working directory
setwd("/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018") # /Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output
currwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018"
headwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/"
# b) Retrieve list of all simulation data files
allfiles <- list.files(
path = currwd, # replace with the directory you want
pattern = "sbickley1_completed_survey_data.*\\.csv$", # filenames ending in "_matches.csv"
full.names = TRUE, # include the directory in the result
recursive = TRUE # scan all subdirectories
)
# c1) Filter out files from 'superseded' directories
allfiles <- allfiles[!grepl("/superseded/", allfiles)]
# c2) Separate file lists into the hotel and gym vignettes (different no. Qs) to process separately and merge together later
list_hotel <- c('1699850405','1699850577','1699850848','1699851006','1699852589',
'1699852841','1699853044','1699853333','1699854457','1699854671',
'1699854822','1699854956','1699855527','1699855740','1699855969',
'1699856134','1699856331')
list_gym <- c('1699859503','1699859787','1699860236','1699860780','1699860961',
'1699861480','1699866531','1699871547','1699873001','1699873281',
'1699873524','1699873822','1699874093','1699874348','1699874494',
'1699874609','1699874889','1699875257','1699875582','1699875946',
'1699930201')
allfiles_hotel <- allfiles[grepl(paste(list_hotel, collapse='|'), allfiles)]
allfiles_gym <- allfiles[grepl(paste(list_gym, collapse='|'), allfiles)]
# ---- FUNCTIONS
# -- a) Function to extract answers and return response type - handles 4 different data structures in "answer" column
extract_answers <- function(data, max_length) {
response <- list(answers = NULL, response_type = NA)
# Check if the data is non-existent or empty
if (length(data)==0) {
response$answers <- c(NA, rep(NA, max_length - 1))
response$response_type <- "nil_response"
}
# Check if only a single answer is provided
else if (length(data)==1) {
response$answers <- c(data, rep(NA, max_length - 1))
response$response_type <- "single_answer"
}
# Check if the data is a named list (i.e., contains "id" and "answer")
else if ("id" %in% tolower(names(data)) && "answer" %in% tolower(names(data))) {
response$answers <- data$answer
response$response_type <- "named_list"
}
# Check if the data is a list of lists with nested "answer" values
else if (is.list(data[[1]]) && "answer" %in% tolower(names(data[[1]]))) {
response$answers <- unlist(lapply(data, function(x) x$answer))
response$response_type <- "list_of_lists"
}
# Handle single character cases
else if (is.character(data) || is.numeric(data)) {
response$answers <- c(data, rep(NA, max_length - 1))
response$response_type <- "single_character"
}
# Otherwise, assume the data is a simple list of answers
else {
response$answers <- data
response$response_type <- "simple_list"
}
return(response)
}
# -- b) # Function to extract answer instructions
extract_instructions <- function(instruction_str) {
# Split the string by newline
lines <- unlist(strsplit(instruction_str, split = "\n"))
# Use regex to extract the actual instruction part after the numbering
instructions <- sapply(lines, function(x) {
if (grepl("^\\d+\\. ", x)) {
sub("^\\d+\\. ", "", x)
} else if (is.na(x)){
NA
} else {
x # Return the original line if it doesn't match the pattern
}
})
# Remove NAs (lines that didn't match the instruction pattern)
return(instructions[!is.na(instructions)])
}
# -- c) # Function to each column to the desired datatype
coerce_columns <- function(df) {
df$Simulation.id <- as.factor(df$Simulation.id)
df$User.id <- as.factor(df$User.id)
df$Status <- as.factor(df$Status)
df$Created <- as.character(df$Created)
#df$average.prompt.tokens <- as.integer(df$average.prompt.tokens)
#df$average.completion.tokens <- as.integer(df$average.completion.tokens)
#df$average.actual.costs <- as.integer(df$average.actual.costs)
df$Agent <- as.integer(df$Agent)
df$Question <- as.factor(df$Question)
df$Question.id <- as.factor(df$Question.id)
#df$number.of.questions <- as.integer(df$number.of.questions)
df$Answer.instruction <- as.character(df$Answer.instruction)
df$Answer <- as.character(df$Answer)
df$Justification <- as.factor(df$Justification)
df$Justification.prompt <- as.character(df$Justification.prompt)
df$Justification.content <- as.character(df$Justification.content)
df$Critic <- as.factor(df$Critic)
df$Critic.prompt <- as.character(df$Critic.prompt)
df$Analysis <- as.character(df$Analysis)
df$Suggestions <- as.character(df$Suggestions)
df$Model <- as.factor(df$Model)
df$Role <- as.factor(df$Role)
df$Temperature <- as.integer(df$Temperature)
df$Age <- as.factor(df$Age)
df$Gender <- as.factor(df$Gender)
df$Education <- as.factor(df$Education)
#df$personality <- as.factor(df$personality)
#df$sexual.orientation <- as.factor(df$sexual.orientation)
#df$race.ethnicity <- as.factor(df$race.ethnicity)
#df$disability <- as.factor(df$disability)
#df$religion <- as.factor(df$religion)
#df$country.of.residence <- as.factor(df$country.of.residence)
#df$household.composition <- as.factor(df$household.composition)
return(df)
}
### -------------- STEP 2: READ IN SIMULATION DATA, MERGE INTO ONE DATAFRAME, CONVERT TO WIDE FORMAT --------------
# -- 0a) Read all csv files then perform row-binding with dplyr
list_of_dataframes_hotel <- lapply(allfiles_hotel, read.csv)
list_of_dataframes_gym <- lapply(allfiles_gym, read.csv)
# -- 0b) Apply the coercion function to each dataframe in the list
list_of_dataframes_hotel <- lapply(list_of_dataframes_hotel, coerce_columns)
list_of_dataframes_gym <- lapply(list_of_dataframes_gym, coerce_columns)
# -- 0c) Bind rows together
combined_df_hotel <- bind_rows(list_of_dataframes_hotel)
combined_df_gym <- bind_rows(list_of_dataframes_gym)
# -- 0d) Aggregate data by 'agent' and 'question.id' and create 'multiple_rows_answered'
aggregated_df_hotel <- combined_df_hotel %>%
group_by(`Simulation.id`, Agent, `Question.id`) %>%
mutate(Answer = paste0("[", paste(Answer, collapse = ","), "]"),
multiple_rows_answered = ifelse(n() > 1, 1, 0)) %>%
slice(1) %>%
ungroup()
aggregated_df_gym <- combined_df_gym %>%
group_by(`Simulation.id`, Agent, `Question.id`) %>%
mutate(Answer = paste0("[", paste(Answer, collapse = ","), "]"),
multiple_rows_answered = ifelse(n() > 1, 1, 0)) %>%
slice(1) %>%
ungroup()
# -- 0e) Remove double opening and closing square brackets
aggregated_df_hotel$Answer <- gsub("\\[\\[", "[", aggregated_df_hotel$Answer)
aggregated_df_hotel$Answer <- gsub("\\]\\]", "]", aggregated_df_hotel$Answer)
aggregated_df_gym$Answer <- gsub("\\[\\[", "[", aggregated_df_gym$Answer)
aggregated_df_gym$Answer <- gsub("\\]\\]", "]", aggregated_df_gym$Answer)
# ---- CLEAN THE ANSWER DATA - START
# -- 0f) Initialize columns with NA
aggregated_df_hotel$list1 <- NA
aggregated_df_hotel$list2 <- NA
aggregated_df_gym$list1 <- NA
aggregated_df_gym$list2 <- NA
# -- 0g) Function to handle aggregated answers
handle_aggregated_answer <- function(answer) {
# Check if answer needs to be wrapped in square brackets
if (startsWith(answer, "[") && grepl("\\],\\[", answer) && endsWith(answer, "]")) {
answer <- paste0("[", answer, "]")
}
# Try parsing, if it fails return the answer unmodified
parsed <- tryCatch({
fromJSON(answer, simplifyVector = FALSE)
}, error = function(e) NULL)
# If parsing failed or result is not a list of lists with exactly two members, return the original answer
if (is.null(parsed) || !is.list(parsed) || length(parsed) != 2 || !all(sapply(parsed, is.list))) {
return(list(answer = answer, list1 = NA, list2 = NA))
}
# Separate the two lists
list1 <- unlist(parsed[[1]])
list2 <- unlist(parsed[[2]])
# Check if the two lists have the same length
if(length(list1) != length(list2)) {
return(list(answer = answer, list1 = list1, list2 = list2))
}
# Compute the average
avg <- mapply(function(x, y) (as.numeric(x) + as.numeric(y)) / 2, list1, list2)
# Convert the average list to the desired character format
avg_string <- paste0("[", paste(avg, collapse = ", "), "]")
return(list(answer = avg_string, list1 = list1, list2 = list2))
}
# -- 0h) Apply the function to the 'answer' column, and bind the list of lists together
result_hotel <- lapply(aggregated_df_hotel$Answer, handle_aggregated_answer)
result_gym <- lapply(aggregated_df_gym$Answer, handle_aggregated_answer)
# -- 0i) Update the dataframe with the results
aggregated_df_hotel$Answer <- sapply(result_hotel, function(x) x$answer)
aggregated_df_hotel$list1 <- sapply(result_hotel, function(x) x$list1)
aggregated_df_hotel$list2 <- sapply(result_hotel, function(x) x$list2)
aggregated_df_gym$Answer <- sapply(result_gym, function(x) x$answer)
aggregated_df_gym$list1 <- sapply(result_gym, function(x) x$list1)
aggregated_df_gym$list2 <- sapply(result_gym, function(x) x$list2)
# -- a) Parse the "answer" column # NOTE - UP TO HERE....
parsed_data_hotel <- lapply(aggregated_df_hotel$Answer, function(x) {
tryCatch({
fromJSON(x, simplifyVector = FALSE)
}, error = function(e) NULL)
})
parsed_data_gym <- lapply(aggregated_df_gym$Answer, function(x) {
tryCatch({
fromJSON(x, simplifyVector = FALSE)
}, error = function(e) NULL)
})
# -- b) Determine the maximum length of answers across all rows
max_length_hotel <- max(sapply(parsed_data_hotel, length))
max_length_gym <- max(sapply(parsed_data_gym, length))
# -- c) Extract answers according to the structure of each parsed object
parsed_answers_hotel <- lapply(parsed_data_hotel, function(x) {
extract_answers(x, max_length_hotel)
})
parsed_answers_gym <- lapply(parsed_data_gym, function(x) {
extract_answers(x, max_length_gym)
})
# -- d) Ensure Each "answers" is of length max_length - padded with NAs
parsed_answers_hotel <- lapply(parsed_answers_hotel, function(item) {
item$answers <- c(item$answers, rep(NA, max_length_hotel - length(item$answers)))
return(item)
})
parsed_answers_gym <- lapply(parsed_answers_gym, function(item) {
item$answers <- c(item$answers, rep(NA, max_length_gym - length(item$answers)))
return(item)
})
# -- e) Convert the parsed answers into a data.frame and add response_type column
answer_df_hotel <- data.frame(matrix(unlist(lapply(parsed_answers_hotel, `[[`, "answers")), ncol=max_length_hotel, byrow=TRUE))
colnames(answer_df_hotel) <- paste0("answer_", seq_len(ncol(answer_df_hotel))) # Rename columns to desired format
response_types_hotel <- unlist(lapply(parsed_answers_hotel, `[[`, "response_type"))
response_types_hotel <- c(response_types_hotel, rep(NA, nrow(answer_df_hotel) - length(response_types_hotel)))
answer_df_hotel$response_type <- response_types_hotel
#answer_df_hotel <- slice(answer_df_hotel, 1:(n() - 1))
answer_df_gym <- data.frame(matrix(unlist(lapply(parsed_answers_gym, `[[`, "answers")), ncol=max_length_gym, byrow=TRUE))
colnames(answer_df_gym) <- paste0("answer_", seq_len(ncol(answer_df_gym))) # Rename columns to desired format
response_types_gym <- unlist(lapply(parsed_answers_gym, `[[`, "response_type"))
response_types_gym <- c(response_types_gym, rep(NA, nrow(answer_df_gym) - length(response_types_gym)))
answer_df_gym$response_type <- response_types_gym
#answer_df_gym <- slice(answer_df_gym, 1:(n() - 1))
# ---- CLEAN THE ANSWER DATA - END
# -- f) Bind the original dataframe with the answer_df
final_df_hotel <- bind_cols(aggregated_df_hotel, answer_df_hotel)
final_df_gym <- bind_cols(aggregated_df_gym, answer_df_gym)
# Change response_type to 'two_lists' for rows where list1 or list2 are non-NA
final_df_hotel$response_type[!is.na(final_df_hotel$list1) | !is.na(final_df_hotel$list2)] <- "two_lists"
final_df_gym$response_type[!is.na(final_df_gym$list1) | !is.na(final_df_gym$list2)] <- "two_lists"
# ---- CLEAN THE INSTRUCTION/QUESTION DATA - START
# -- g) Extract answer instructions for each row
parsed_instructions_hotel <- lapply(aggregated_df_hotel$`Answer.instruction`, extract_instructions)
parsed_instructions_gym <- lapply(aggregated_df_gym$`Answer.instruction`, extract_instructions)
# -- h) Determine the max number of instructions across rows
max_instructions_hotel <- max(sapply(parsed_instructions_hotel, length))
max_instructions_gym <- max(sapply(parsed_instructions_gym, length))
# -- i) Convert the list to a dataframe with wide format
# Step 1: Initialize a new list to store the modified instructions
modified_instructions_hotel <- vector("list", length = length(parsed_instructions_hotel))
# Step 2: Ensure each element is a list of length 17
modified_instructions_hotel <- lapply(parsed_instructions_hotel, function(x) {
# If the element is not a list, convert it to a list
if (is.character(x)){
x <- unlist(strsplit(x, "\\d+\\.\\s*"))
x <- unique(x[nzchar(x)])
x <- as.list(x)
} else if (!is.list(x)) {
x <- list(x)
}
# Pad the list with NAs to make its length 17
length(x) <- max_instructions_hotel
return(x)
})
# Step 3:Combine all lists into a data frame
instruction_df_hotel <- do.call(rbind, modified_instructions_hotel)
colnames(instruction_df_hotel) <- paste0("instruction_", 1:max_instructions_hotel)
# Step 4: Convert the result into a data frame
instruction_df_hotel <- as.data.frame(instruction_df_hotel)
# Step 5: Initialize a new list to store the modified instructions
modified_instructions_gym <- vector("list", length = length(parsed_instructions_gym))
# Step 6: Ensure each element is a list of length 17
modified_instructions_gym <- lapply(parsed_instructions_gym, function(x) {
# If the element is not a list, convert it to a list
if (is.character(x)){
x <- unlist(strsplit(x, "\\d+\\.\\s*"))
x <- unique(x[nzchar(x)])
x <- as.list(x)
} else if (!is.list(x)) {
x <- list(x)
}
# Pad the list with NAs to make its length 17
length(x) <- max_instructions_gym
return(x)
})
# Step 7: Combine all lists into a data frame
instruction_df_gym <- do.call(rbind, modified_instructions_gym)
colnames(instruction_df_gym) <- paste0("instruction_", 1:max_instructions_gym)
# Step 8: Convert the result into a data frame
instruction_df_gym <- as.data.frame(instruction_df_gym)
# ---- CLEAN THE INSTRUCTION/QUESTION DATA - END
# -- j) Combine the new instruction columns with the original dataframe
final_df_hotel <- bind_cols(final_df_hotel, instruction_df_hotel)
final_df_gym <- bind_cols(final_df_gym, instruction_df_gym)
# -- k1) Get the subset of columns that start with "answer_" and "instruction_" and convert to numeric and character, respectively
answer_columns_hotel <- grep("^answer_", colnames(final_df_hotel), value = TRUE)
final_df_hotel[, answer_columns_hotel] <- lapply(final_df_hotel[, answer_columns_hotel], as.numeric) # Convert those columns to numeric
answer_columns_gym <- grep("^answer_", colnames(final_df_gym), value = TRUE)
final_df_gym[, answer_columns_gym] <- lapply(final_df_gym[, answer_columns_gym], as.numeric) # Convert those columns to numeric
instruction_columns_hotel <- grep("^instruction_", colnames(final_df_hotel), value = TRUE)
final_df_hotel[, instruction_columns_hotel] <- lapply(final_df_hotel[, instruction_columns_hotel], as.character) # Convert those columns to character
instruction_columns_gym <- grep("^instruction_", colnames(final_df_gym), value = TRUE)
final_df_gym[, instruction_columns_gym] <- lapply(final_df_gym[, instruction_columns_gym], as.character) # Convert those columns to character
# -- k2) Change the response_type where multiple_rows_answered is greater than 0, and Drop the multiple_rows_answered column
final_df_hotel$response_type[final_df_hotel$multiple_rows_answered > 0] <- "multiple_lines"
final_df_hotel$multiple_rows_answered <- NULL
final_df_gym$response_type[final_df_gym$multiple_rows_answered > 0] <- "multiple_lines"
final_df_gym$multiple_rows_answered <- NULL
# -- l1) Calculate the number of non-NA values for each row in the subset of columns, and save to xlsx file (OPTIONAL)
final_df_hotel$non_na_counts <- apply(final_df_hotel[, answer_columns_hotel], MARGIN = 1, function(x) sum(!is.na(x)))
final_df_gym$non_na_counts <- apply(final_df_gym[, answer_columns_gym], MARGIN = 1, function(x) sum(!is.na(x)))
# -- l2) Join the two final_df datasets back to each other
# Step 1: Add missing columns to final_df_hotel with NaN values
missing_answer_cols <- paste0("answer_", 17:24)
missing_instruction_cols <- paste0("instruction_", 18:24)
final_df_hotel[missing_answer_cols] <- NaN
final_df_hotel[missing_instruction_cols] <- NaN
# Step 2: Reorder columns of final_df_hotel to match final_df_gym
final_df_hotel <- final_df_hotel[colnames(final_df_gym)]
# Step 3: Combine the dataframes by rows
final_combined_df <- rbind(final_df_hotel, final_df_gym)
instruction_columns <- grep("^instruction_", colnames(final_combined_df), value = TRUE)
final_combined_df[, instruction_columns] <- lapply(final_combined_df[, instruction_columns], as.character) # Convert those columns to character
answer_column <- grep("^answer_", colnames(final_combined_df), value = TRUE)
final_combined_df[, answer_column] <- lapply(final_combined_df[, answer_column], as.numeric) # Convert those columns to character
# Step 4: Create new "total_count" column
final_combined_df$total_count <- ifelse(grepl("HOTEL", final_combined_df$Question.id), 16, 24) # add in the total number of expected responses
# Step 5: Modify 'response_type' based on conditions
final_combined_df$response_type <- ifelse(
final_combined_df$response_type == "single_answer" & grepl("T3V6", final_combined_df$Question.id),
"single_answer_compliant",
ifelse(final_combined_df$response_type == "single_answer", "single_answer_noncompliant", final_combined_df$response_type)
)
# Step 6: (Optional) Save to xlsx file
#write_xlsx(final_combined_df, path="combined_study1_&_study2_v1.xlsx")
rm(result_hotel, result_gym, answer_df_hotel, answer_df_gym, combined_df_hotel, combined_df_gym, aggregated_df_hotel, aggregated_df_gym, instruction_df_hotel, instruction_df_gym, list_of_dataframes_hotel, list_of_dataframes_gym, parsed_answers_hotel, parsed_answers_gym, parsed_data_hotel, parsed_data_gym, parsed_instructions_hotel, parsed_instructions_gym)
rm(max_instructions_hotel, max_instructions_gym, max_length_hotel, max_length_gym, extract_answers, coerce_columns, handle_aggregated_answer, extract_instructions, answer_columns_hotel, answer_columns_gym, response_types_hotel, response_types_gym)
rm(final_df_gym, final_df_hotel, modified_instructions_gym, modified_instructions_hotel)
rm(allfiles,answer_column,instruction_columns,instruction_columns_gym,instruction_columns_hotel,list_gym,list_hotel)
rm(missing_answer_cols,missing_instruction_cols)
# -- l3) Next, clean up the "single_answer_compliant" rows to merge them into single, unique rows with all answer_ and instruction_ columns in one place
# Step 1: Extract the base condition and the identifier
compliant_df <- final_combined_df %>%
filter(response_type == "single_answer_compliant") %>%
mutate(base_condition = str_replace(Question.id, "_[^_]*$", ""),
question_id = str_extract(Question.id, "[^_]*$"))
compliant_df <- compliant_df %>% mutate(identifier = str_sub(question_id, -1, -1))
# Step 2a: Reshape the "compliant_df" dataframe
processed_df <- compliant_df %>%
group_by(Simulation.id, Agent, base_condition) %>%
summarise(across(starts_with("answer_"), list),
across(starts_with("instruction_"), list),
.groups = "drop")
# Step 2b: Add "answer_base" and "instruction_base" columns
processed_df <- processed_df %>%
group_by(Simulation.id, Agent, base_condition) %>%
mutate(answer_base = answer_1, instruction_base = instruction_1)
# Step 2c: Define a function to pad a list to a specified length with NaNs
pad_list_to_length <- function(lst, length) {
len <- length(lst)
if (len < length) {
return(c(lst, rep(NaN, length - len)))
} else {
return(lst)
}
}
# Step 2d: Apply the function to each element of 'answer_base' and 'instruction_base'
processed_df <- processed_df %>%
group_by(Simulation.id, Agent, base_condition) %>%
mutate(
answer_base = lapply(answer_base, pad_list_to_length, length = 24),
instruction_base = lapply(instruction_base, pad_list_to_length, length = 24)
)
library(purrr)
# Step 2e: Function to unlist and expand the columns
expand_column <- function(column, prefix) {
# Unlist and create a dataframe
column_df <- map_df(column, ~as.data.frame(t(.)))
# Rename the columns
names(column_df) <- paste0(prefix, 1:24)
return(column_df)
}
# Step 2f: Apply the function to 'answer_base' and 'instruction_base' columns
answer_df <- expand_column(processed_df$answer_base, "answer_")
answer_base <- processed_df$answer_base
instruction_df <- expand_column(processed_df$instruction_base, "instruction_")
instruction_base <- processed_df$instruction_base
# Step 2g: Remove existing 'answer_' and 'instruction_' columns from processed_df
answer_cols <- grep("^answer_", names(processed_df), value = TRUE)
instruction_cols <- grep("^instruction_", names(processed_df), value = TRUE)
processed_df <- select(processed_df, -all_of(answer_cols), -all_of(instruction_cols)) # Remove existing 'answer_' and 'instruction_' columns from processed_df
# Step 2h: Bind these dataframes back to the original dataframe and add "answer_base" and "instruction_base" columns back in
processed_df <- cbind(processed_df, answer_df, instruction_df)
processed_df$answer_base <- answer_base
processed_df$instruction_base <- instruction_base
rm(answer_base,answer_df,compliant_df,instruction_base,instruction_df,answer_cols,instruction_cols,expand_column,pad_list_to_length)
# -- l4) Finally, recombine the clean data from "processed_df" for the "single_answer_compliant" row back into the main dataset
# Step 1: Create "base_condition" column in final_combined_df
final_combined_df <- final_combined_df %>%
mutate(base_condition = str_replace(Question.id, "_[^_]*$", ""))
# Step 2: Identify and remove redundant rows in final_combined_df
# For each group, we keep the first row as the representative
final_combined_df <- final_combined_df %>%
group_by(Simulation.id, Agent, base_condition) %>%
mutate(row_number = row_number()) %>%
ungroup() %>%
filter(!(response_type == "single_answer_compliant" & row_number > 1)) %>%
select(-row_number)
# Step 3: Update only the "answer_" and "instruction_" columns in final_combined_df using processed_df
# -- i) Join final_df and processed_df
final_df <- final_combined_df %>%
left_join(processed_df, by = c("Simulation.id", "Agent", "base_condition"), suffix = c("", ".new"))
# -- ii) Replace the existing Answer column for rows, where response_type == "single_answer_compliant", with "answer_base"
final_df <- final_df %>% mutate(Answer = ifelse(response_type == "single_answer_compliant", answer_base, Answer))
final_df <- final_df[,!names(final_df) %in% c("answer_base")]
# -- iii) Update the answer_ and instruction_ columns
answer_cols <- grep("^answer_", names(final_df), value = TRUE)
instruction_cols <- head(grep("^instruction_", names(final_df), value = TRUE),-1)
for (col_name in c(answer_cols, instruction_cols)) {
new_col_name <- paste0(col_name, ".new")
final_df <- final_df %>%
mutate(!!col_name := coalesce(.[[new_col_name]], .[[col_name]]))
}
# -- iv) Remove the new columns and old data/values in working space as well
final_df <- select(final_df, -matches("\\.new$"))
rm(final_combined_df,processed_df,answer_cols,col_name,instruction_cols,answer_cols,new_col_name)
# Step 4: Recalculate 'non_na_counts' for rows with 'single_answer_compliant'
final_df <- final_df %>%
mutate(non_na_counts = ifelse(response_type == "single_answer_compliant", rowSums(!is.na(select(., starts_with("answer_")))), non_na_counts))
# Step 5: Extract the question_id from "Question.id" column and add this as new column to "final_df"
final_df <- final_df %>%
mutate(question_id = str_extract(Question.id, "[^_]*$"))
final_df <- final_df %>% mutate(question_id = ifelse(response_type == "single_answer_compliant" | grepl("T3V5", question_id), str_sub(question_id, end = -2), question_id)) # clean up question_id's for T3V5 {grepl("T3V5", question_id)} and T3V6 {response_type == "single_answer_compliant")} in particular
# Step 6: (Optional) Save to xlsx file and check final_df looks right
write_xlsx(final_df, path="combined_study1_&_study2_v2.xlsx")
#test_df <- final_df %>% filter(response_type == "single_answer_compliant")
# -- m) Subset the dataframe to keep rows where there's more than one non-NA value
analysis_df <- final_df[final_df$non_na_counts > 1 | (final_df$response_type == "single_answer_compliant" & final_df$non_na_counts >= 1), ]
#analysis_df <- final_df[final_df$non_na_counts > 1, ]
# -- n) Summary stats - count the number of responses for each unique value in the question.id column
print(n=34, final_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id))))
print(n=34, analysis_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id))))
write_xlsx(final_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id))), path="combined_study1_&_study2_summary_v1.xlsx")
print(n=34, final_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id)),response_types = toString(unique(response_type))))
write_xlsx(final_df %>%  group_by(base_condition, question_id) %>%   summarise(response_count = n(),simulation_ids = toString(unique(Simulation.id)),response_types = toString(unique(response_type))), path="combined_study1_&_study2_summary_v1.xlsx")
rm(list = ls())
#install.packages("httr")
library("httr")
library("data.table")
library("dplyr")
library("lubridate")
library("writexl")
library("jsonlite")
library("readxl")
library("stringr")
library("tidyr")
### -------------- STEP 1: SETUP AND DEFINE FUNCTIONS --------------
# ---- SETUP
# i) Set file paths for study
# Blursday Thematic Coding Study
blurswd <- "/Users/stevenbickley/stevejbickley/blursday_assistant"
# NOTE:
# 1) File path for Blursday Thematic Coding Study: "/Users/stevenbickley/stevejbickley/blursday_assistant"
# a) Set current working directory
currwd <- blurswd
setwd(currwd)
headwd <- blurswd
allfiles <- list.files(
path = currwd, # replace with the directory you want
pattern = "parsed_responses_final.*\\.xlsx$",
full.names = TRUE, # include the directory in the result
recursive = TRUE # scan all subdirectories
)
# c1) Filter out files from 'superseded' directories
allfiles <- allfiles[!grepl("/superseded/", allfiles)]
length(allfiles)
